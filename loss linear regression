/*

LINEAR REGRESSION
Loss
When we think about how we can assign a slope and intercept to fit a set of points, we have to define what the best fit is.

For each data point, we calculate loss, a number that measures how bad the model’s (in this case, the line’s) prediction was. You may have seen this being referred to as error.

We can think about loss as the squared distance from the point to the line. We do the squared distance (instead of just the distance) so that points above and below the line both contribute to total loss in the same way:

*/

x = [1, 2, 3]
y = [5, 1, 3]

#y = x
m1 = 1
b1 = 0

#y = 0.5x + 1
m2 = 0.5
b2 = 1

y_predicted1 = [m1*x_val + b1 for x_val in x]
y_predicted2 = [m2*x_val + b2 for x_val in x]




total_loss1 = 0
total_loss2 = 0

for i in range(len(y)):
 total_loss1 += (y[i] - y_predicted1[i])**2
 total_loss2 += (y[i] - y_predicted2[i])**2


  
print(total_loss1, total_loss2)
better_fit = 2
